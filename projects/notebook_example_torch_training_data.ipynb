{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kJpfFo51LMVd",
        "eAxs5_GvLZdO",
        "OfGY5sUfLaoO",
        "Vk5x-anaLvaZ",
        "W6m8EBw_LzaY"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CIA-Oceanix/DLCourse_MOi_2022/blob/main/projects/notebook_example_torch_training_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import des données altimétriques"
      ],
      "metadata": {
        "id": "e7Lc6Kwp4-Q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "BT6rmJLDK_XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xarray[complete] eccodes -q"
      ],
      "metadata": {
        "id": "YsxhX3af8X60",
        "outputId": "ecf2920b-b8d3-48d9-c068-1a364b0d0d2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 56 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 185 kB 57.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 355 kB 53.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 39.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 114 kB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 10.1 MB/s \n",
            "\u001b[?25h  Building wheel for eccodes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for findlibs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download netcdfs"
      ],
      "metadata": {
        "id": "E2ij8zjHLErh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download NATL60 data"
      ],
      "metadata": {
        "id": "kJpfFo51LMVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://s3.us-east-1.wasabisys.com/melody/osse_data/ref/NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv1tciR6_RD1",
        "outputId": "d4991acc-fcdc-4a6c-b632-4d9d6007360b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-04 07:31:40--  https://s3.us-east-1.wasabisys.com/melody/osse_data/ref/NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc\n",
            "Resolving s3.us-east-1.wasabisys.com (s3.us-east-1.wasabisys.com)... 38.27.106.51, 38.27.106.53\n",
            "Connecting to s3.us-east-1.wasabisys.com (s3.us-east-1.wasabisys.com)|38.27.106.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://s3.eu-central-1.wasabisys.com/melody/osse_data/ref/NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc [following]\n",
            "--2022-10-04 07:31:40--  https://s3.eu-central-1.wasabisys.com/melody/osse_data/ref/NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc\n",
            "Resolving s3.eu-central-1.wasabisys.com (s3.eu-central-1.wasabisys.com)... 130.117.252.28, 130.117.252.10, 130.117.252.32, ...\n",
            "Connecting to s3.eu-central-1.wasabisys.com (s3.eu-central-1.wasabisys.com)|130.117.252.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118023544 (113M) [application/x-netcdf]\n",
            "Saving to: ‘NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc’\n",
            "\n",
            "NATL60-CJM165_GULFS 100%[===================>] 112.56M  13.2MB/s    in 10s     \n",
            "\n",
            "2022-10-04 07:31:52 (10.8 MB/s) - ‘NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc’ saved [118023544/118023544]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example torch dataloading\n",
        "\n",
        "## Concepts\n",
        "\n",
        "In order to create the training data two torch classes are used:\n",
        "\n",
        "- torch.utils.data.Dataset: collection of elementary item or items for one training iteration\n",
        "    - Example: `(x, y)`: 1 image `x` + corresponding label `y`\n",
        "    - Example forcasting: `(ssh_t, ssh_t_plus_1)`: passed ssh  + future ssh\n",
        "    - Example downscaling: `(ssh_low_res, ssh_high_res)`: ...\n",
        "\n",
        "- torch.utils.data.DataLoader: iterable that takes a `Dataset` as input and a `batch_size` and constitute the \"batches\": \n",
        "    - Example if the dataset return `(x, y)` with `x~(channel, height, width)` and `y~(label)`, `Dataloader(dataset, batch_size=batch_size)` will return (bx, by) with  with `bx~(batch_size, channel, height, width)` and `by~(batch_size, label)`\n",
        "\n",
        "\n",
        "## Practical considerations\n",
        "\n",
        "In order to create a dataset a simple way is to stack all training items along the first dimension and then instantiate a `torch.utils.data.TensorDataset` below an example of a dataset that return the `(ssh_t, ssh_t_plus_1)` "
      ],
      "metadata": {
        "id": "-PZSgepy6J-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import xarray as xr\n",
        "\n",
        "# create stacked items\n",
        "ref_ds = xr.open_dataset('NATL60-CJM165_GULFSTREAM_ssh_y2013.1y.nc', decode_times=False).assign_coords(time=lambda ds: pd.to_datetime(ds.time))\n",
        "\n",
        "ssh_t = ref_ds.ssh.isel(time=slice(None, -1)) # t0 ... tN-1\n",
        "ssh_t_plus_1 = ref_ds.ssh.isel(time=slice(1, None)) # t1 ... tN\n",
        "\n",
        "# convert to torch tensor\n",
        "ssh_t_tensor = torch.from_numpy(ssh_t.values)\n",
        "ssh_t_plus_1_tensor = torch.from_numpy(ssh_t_plus_1.values)\n",
        "\n",
        "# Create dataset\n",
        "torch_dataset = torch.utils.data.TensorDataset(ssh_t_tensor, ssh_t_plus_1_tensor)\n",
        "\n",
        "example_item = torch_dataset[0]\n",
        "ssh_t_item, ssh_t_plus_1_item = example_item\n",
        "print(\"Item sizes\")\n",
        "print(f'ssh_t_item, {ssh_t_item.size()}')\n",
        "print(f'ssh_t_plus_1_item, {ssh_t_plus_1_item.size()}')\n",
        "print()\n",
        "\n",
        "# Create dataloader\n",
        "torch_dataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=8)\n",
        "\n",
        "print(\"Batch sizes\")\n",
        "for example_batch in torch_dataloader:\n",
        "    break\n",
        "ssh_t_batch, ssh_t_plus_1_batch = example_batch\n",
        "print(f'ssh_t_batch, {ssh_t_batch.size()}')\n",
        "print(f'ssh_t_plus_1_batch, {ssh_t_plus_1_batch.size()}')"
      ],
      "metadata": {
        "id": "6g72vwH96oIv",
        "outputId": "8227016b-5911-48d2-87d2-5e224e103040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item sizes\n",
            "ssh_t_item, torch.Size([201, 201])\n",
            "ssh_t_plus_1_item, torch.Size([201, 201])\n",
            "\n",
            "Batch sizes\n",
            "ssh_t_batch, torch.Size([8, 201, 201])\n",
            "ssh_t_plus_1_batch, torch.Size([8, 201, 201])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TO DO FOR train validation and test datasets :)"
      ],
      "metadata": {
        "id": "_CDYwNB76U4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROTISfq3GImr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}